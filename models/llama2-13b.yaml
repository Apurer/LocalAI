backend: llama
name: llama2-13b
parameters:
  model: llama-2-13b-chat.Q5_K_M.gguf
  top_k: 80
  temperature: 0.2
  top_p: 0.7
context_size: 4000
f16: true ## If you are using cpu set this to false
gpu_layers: 60
stopwords:
- "HUMAN:"
- "GPT:"
roles:
  user: " "
  system: " "
template:
  chat: llama2-13b-chat
  completion: llama2-13b-completion